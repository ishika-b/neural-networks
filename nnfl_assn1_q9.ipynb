{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfl_assn1_q9.ipynb",
      "provenance": [],
      "mount_file_id": "1sxyjwNGqVJgF9NVCPjbJo1RBqlPD8v60",
      "authorship_tag": "ABX9TyO4bhFFxajyzzgXtfSkTnNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishika-b/neural-networks/blob/main/nnfl_assn1_q9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mzn53sa-MxM"
      },
      "source": [
        "import random \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro0AsHWmGO6I"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cost(h, y):\n",
        "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
        "    \n",
        "def predict(x,w):\n",
        "    return sigmoid(np.dot(x,w))\n",
        "\n",
        "def gradient_descent(x,y,num_iter,alpha):   \n",
        "    # weights initialization\n",
        "    w = np.zeros((8,3)) #number of weights(w7..w0) x number of classes(0,1,2)\n",
        "    costs = np.zeros((num_iter,4)) #array of number of iterations x (num_classes)\n",
        "        \n",
        "    for i in range(num_iter):\n",
        "      costs[i][0]=i #0 column has value of iteration for plotting\n",
        "      for j in range(3): #run a loop for each cluster\n",
        "        #x(instances,features)*w(features,a particular class)=(instances,hypothesis for that instance and that cluster)\n",
        "        hypothesis = sigmoid(np.dot(x, w[:,j]))\n",
        "        costs[i][j+1] = cost(hypothesis, y[:,j])\n",
        "        #update the weights\n",
        "        gradient = np.dot(x.T, (hypothesis - y[:,j])) / x.shape[0]\n",
        "        w[:,j] = w[:,j] - alpha * gradient\n",
        "    return costs,w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZENMzO2JPAfA"
      },
      "source": [
        "def onevall(x_train,y_train,x_test,y_test):\n",
        "  costs,w = gradient_descent(x_train,y_train,1000,0.02)\n",
        "\n",
        "  predictions=[]\n",
        "  predicted_values = np.zeros(30)\n",
        "  for i in range(30):\n",
        "    for j in range(3):\n",
        "      predictions.append(sigmoid(np.dot(x_test[i],w[:,j])))\n",
        "    predicted_values[i] = predictions.index(max(predictions))+1\n",
        "    predictions = []\n",
        "  equal=0\n",
        "  total=np.zeros(3)\n",
        "  #Accuracy measures\n",
        "  acc=[]\n",
        "  acc_ind=np.zeros(3)\n",
        "  for i in range(30):\n",
        "    if (predicted_values[i]==y_test[i]):\n",
        "      equal = equal+1\n",
        "    for j in range(3):\n",
        "      if ((predicted_values[i]==j+1) and (y_test[i]==j+1)):\n",
        "        acc_ind[j] = acc_ind[j]+1\n",
        "      if y_test[i]==j+1:\n",
        "        total[j]=total[j]+1\n",
        "\n",
        "  accuracy = equal/30\n",
        "\n",
        "  acc_ind=np.divide(acc_ind,total)\n",
        "  return acc_ind,accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WBFIcXZN0yl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "9f9923ce-863d-4ae8-c106-bf5b203fda2f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  np.random.seed()\n",
        "  ex = pd.read_excel('/content/drive/My Drive/nnfl/nnfl/data4.xlsx', sheet_name='Sheet1',header=None)\n",
        "  ex1 = ex.sample(frac=1).reset_index(drop=True)\n",
        "  #ex1 is 150*8\n",
        "\n",
        "  x = np.zeros((150,7))\n",
        "  y = np.zeros((150,1))\n",
        "  folds = 5\n",
        "  \n",
        "  x = np.array(ex1)[:,:-1]\n",
        "  y = np.array(ex1)[:,7:]\n",
        "\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "  for i in range(7):\n",
        "    mean = np.mean(x[:,i])\n",
        "    std = np.std(x[:,i])\n",
        "    x[:,i] = [(a-mean)/std for a in x[:,i]]\n",
        "  x = np.insert(x,0,np.ones(150),axis=1)\n",
        "\n",
        "  fold_size = int(150/folds)\n",
        "  accuracy = 0\n",
        "  acc_ind = np.zeros(3)\n",
        "\n",
        "  for k in range(folds):\n",
        "    x_train = np.zeros((120,8))\n",
        "    x_test = np.zeros((30,8))\n",
        "    y_test = np.zeros((30,1))\n",
        "    y_train = np.zeros((120,3),dtype=int)\n",
        "    y_tr = np.zeros((120,1),dtype=int)\n",
        "    x_test = x[k*fold_size:(k+1)*fold_size][:]\n",
        "    x_train[0:k*fold_size][:] = x[0:k*fold_size][:]\n",
        "    x_train[k*fold_size:120][:] = x[(k+1)*fold_size:150][:]\n",
        "\n",
        "    y_test = y[k*fold_size:(k+1)*fold_size][:]\n",
        "    \n",
        "    y_tr[0:k*fold_size][:]=y[0:k*fold_size][:]\n",
        "    y_tr[k*fold_size:120][:]=y[(k+1)*fold_size:150][:]\n",
        "\n",
        "    for i in range(120):\n",
        "      index = int(y_tr[i]-1)\n",
        "      y_train[i][index] = 1\n",
        "    a,b = onevall(x_train,y_train,x_test,y_test)\n",
        "    print('\\nFold ',(k+1))\n",
        "    print('Individual Accuracies: ')\n",
        "    print('Class 1:', a[0])\n",
        "    print('Class 2:', a[1])\n",
        "    print('Class 3:', a[2])\n",
        "\n",
        "    print('Accuracy ',b)\n",
        "    acc_ind = acc_ind+a\n",
        "    accuracy = accuracy + b\n",
        "  acc_ind = acc_ind/folds\n",
        "  accuracy = accuracy/folds\n",
        "\n",
        "  print('\\nOverall accuracy = {:0.2f}%'.format(accuracy*100))\n",
        "  print('Individual accuracies:')\n",
        "  print('Class 1:', acc_ind[0])\n",
        "  print('Class 2:', acc_ind[1])\n",
        "  print('Class 3:', acc_ind[2])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 7) (150, 1)\n",
            "\n",
            "Fold  1\n",
            "Individual Accuracies: \n",
            "Class 1: 1.0\n",
            "Class 2: 0.6666666666666666\n",
            "Class 3: 0.9\n",
            "Accuracy  0.8666666666666667\n",
            "\n",
            "Fold  2\n",
            "Individual Accuracies: \n",
            "Class 1: 1.0\n",
            "Class 2: 0.7272727272727273\n",
            "Class 3: 1.0\n",
            "Accuracy  0.9\n",
            "\n",
            "Fold  3\n",
            "Individual Accuracies: \n",
            "Class 1: 1.0\n",
            "Class 2: 0.8461538461538461\n",
            "Class 3: 1.0\n",
            "Accuracy  0.9333333333333333\n",
            "\n",
            "Fold  4\n",
            "Individual Accuracies: \n",
            "Class 1: 1.0\n",
            "Class 2: 0.8\n",
            "Class 3: 0.9230769230769231\n",
            "Accuracy  0.9333333333333333\n",
            "\n",
            "Fold  5\n",
            "Individual Accuracies: \n",
            "Class 1: 1.0\n",
            "Class 2: 0.9166666666666666\n",
            "Class 3: 0.8\n",
            "Accuracy  0.9\n",
            "\n",
            "Overall accuracy = 90.67%\n",
            "Individual accuracies:\n",
            "Class 1: 100.0\n",
            "Class 2: 79.13519813519814\n",
            "Class 3: 92.46153846153845\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}